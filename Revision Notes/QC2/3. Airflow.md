# Apache Airflow

Apache Airflow, initially developed by Airbnb, is a robust open-source platform designed for orchestrating complex workflows (or ETL)

It is used in the sequentialization and automation of tasks within a workflow.

- provides pipeline as a code
- other enterprise tools - altrix, informatica
- airflow provides customization of workflow

### Purpose

The primary purpose of Airflow is to manage workflows (or DAG), which are essentially sequences of tasks. These tasks could range from simple data operations to complex ETL (Extract, Transform, Load) processes. 

### Characteristics of Airflow

1. **Scalable**
- Airflow's scalability is a standout feature, accommodating various workloads and task complexities.

2. **Dynamic**
- The platform supports multiple pipelines, allowing for the creation of dynamic and flexible workflows.

3. **Extensible**
- Airflow can seamlessly integrate with various tools and technologies, making it highly extensible.

4. **Elegant**
- Leveraging Jinja, a templating engine, Airflow enhances code readability and presentation, particularly in documentation and code blocks.

### Features

- **Python Integration:** Airflow utilizes standard Python, making it accessible to Python users and promoting code reuse.

- **User-Friendly Interface:** The platform provides a user-friendly interface for designing and monitoring workflows.

- **API Integration:** Well-integrated with numerous APIs, Airflow facilitates interactions with various services.

- **Open Source:** Airflow is open-source, providing users with flexibility and avoiding licensing constraints.

### Use Cases

- **ETL Pipeline:** Airflow is widely employed for building robust ETL pipelines, ensuring efficient data processing.

- **MLOps (Machine Learning Operations):** It plays a crucial role in orchestrating tasks related to machine learning model deployment and monitoring.

- **Operational Analytics:** Airflow supports the automation of analytical processes, enhancing operational efficiency.

## Core Concepts

- **DAG (Directed Acyclic Graph):** Represents the data flow in the form of a graph.

- **DAG Run:** Execution of a DAG, either triggered manually or through automation.

- **Task:** A single unit of work within a workflow, designed to be independent.

- **Task Instance:** The instantiation of a task at a specific point in time, triggered by a task.

## Workflow Components

![Alt text](<Screenshot 2024-01-18 204541.png>)

- **Webserver:** Provides a user interface for designing and monitoring workflows.

- **Scheduler:** Orchestrates task execution based on defined dependencies and schedules.

- **Database:** Stores metadata related to workflows, tasks, and their status.

- **Executor:** Executes the actual operations defined in tasks.

- **Triggerer:** Initiates workflow execution based on predefined triggers.

- **Worker:** Responsible for executing tasks on worker nodes.

## Operators in Airflow

Operators are essential components attached to each task in a workflow. 
```
To create a task, we use operator.
Think of it as a function provided by airflow to create a task instance.

When you define a task in a Directed Acyclic Graph (DAG) using an operator, you are essentially creating a task instance associated with that operator.

Operators define the type of task and its behavior, and task instances represent the actual executions.
```

**Categories** of operators:
- **Action Operator:** Functionally oriented operators.
- **Transfer Operator:** Facilitates data movement between different points.
- **Sensor Operator:** Monitors events as independent entities.

### Most Frequently Used Operators

- **PythonOperator:** Executes a Python function within a task.

- **BashOperator:** Executes a bash script as part of a task.

- **KubernetesPodOperator:** Runs a task defined as a Docker image in a Kubernetes Pod.

- **SnowflakeOperator:** Executes queries against a Snowflake database.



### Q. default vs non default operators.

-  operators are building blocks that define individual tasks within a Directed Acyclic Graph (DAG). 

**Default Operators or Built-in Operators:**

built-in or default operators cover a wide range of common use cases. These are ready to use without additional installation or configuration.

Examples of Default Operators:

- BashOperator: Executes a Bash command.
- PythonOperator: Calls an arbitrary Python function.
- EmailOperator: Sends an email.
- HttpSensor: Waits for an HTTP endpoint to return a response.
- DummyOperator: Represents a no-op task (useful for testing and structure).

**Non-Default Operators Custom or Third-Party Operators:**

those that are not part of the built-in set and are typically developed by the user or the community.

Examples of Non-Default Operators:

- Custom operators developed by your team for domain-specific tasks.
- Third-party operators obtained from external sources or libraries.

## Executors in DAG

Executors in Apache Airflow determine how tasks within a DAG (Directed Acyclic Graph) will run. The choice of executor impacts the parallelism and distribution of task execution.

### Types of Executors:

1. **Sequential Executor:**

- **Description:**
  - Executes tasks sequentially in a single process.
  - Useful for debugging and testing as it simulates a single-threaded environment.

- **Usage:**
  - Suitable for small-scale or development environments where parallelism is not a priority.

2. **Local Executor:**

- **Description:**
  - Executes tasks in parallel using multiple local processes.
  - Each task runs independently in its own process.

- **Usage:**
  - Provides a balance between simplicity and parallelism for medium-sized workflows.
  - Suitable for environments with moderate computational resources.

3. **Celery Executor:**

- **Description:**
  - Distributes tasks across multiple worker nodes using Celery, an open-source distributed task queue.
  - Enables parallel and distributed execution of tasks.

- **Usage:**
  - Ideal for large-scale workflows or when tasks require substantial computational resources.
  - Allows scaling horizontally by adding more Celery worker nodes.

### Configuration:

- The executor type is configured in the Airflow configuration file (`airflow.cfg`) under the `executor` parameter.


## Terminologies for interview

- **DAG:** Represents the flow of data, ensuring a structured and organized workflow.

- **Worker Node:** Executes tasks defined in the workflow.

- **Task Run:** Initiation of a task or execution of an executor.

- **Scheduler:** Manages the scheduling of tasks based on predefined dependencies.


### Airflow Concepts in discussion

1. **Variable in Airflow:**
   - *Definition:* Variables in Airflow are key-value pairs that you can use to store and retrieve arbitrary data.
   - *Usage:* Useful for storing configuration settings, connection strings, or any dynamic values that can be reused across tasks.

2. **Types of Executors in Airflow:**
   - *LocalExecutor:* Executes tasks sequentially in a single process.
   - *CeleryExecutor:* Distributes tasks across multiple worker nodes using Celery.
   - *SequentialExecutor:* Executes tasks sequentially, primarily for debugging and testing.

3. **Hooks in Airflow:**
   - *Definition:* Hooks in Airflow are used to connect and interact with external systems or databases.
   - *Usage:* Provide a Pythonic interface to external systems, making it easier to integrate tasks with various services.

### YAML Variables:

4. **Variable Types in YAML:**
   - *Mapping:* Key-value pairs (similar to dictionaries in Python).
   - *String:* Represents a sequence of characters.
   - *List:* Ordered collection of items.
   - *Multiline String:* Represents a string with multiple lines.

### Docker Usage:

5. **Why Do You Need Docker:**
   - *Containerization:* Docker allows you to package dags and their dependencies into isolated containers.
   - *Portability and simplified deployments:* Encapsulation of dependencies ensures consistency across different environments.
   - *Rollback:* Docker images are versioned, enabling easy rollback to previous versions in case of issues.

### Airflow DAG and Task Naming:

6. **`catchup=False` in Code:**
   - *Purpose:* Setting `catchup=False` in a DAG ensures that only the most recent execution of the DAG is considered, ignoring any historical runs.
   - *Use Case:* Useful when you don't want to execute tasks for historical dates when the DAG is first introduced.

7. **DAG and Task Naming:**
   - *DAG Name Uniqueness:* Each DAG should have a unique name in your Airflow environment (for avoiding conflicts)
   - *Task Name Uniqueness:* Each task within a DAG should also have a unique name.


# Apache Aiflow App Overview based on curriculum

## DAG

### DAG status

- Active: Currently operational and processing tasks.
- Paused: User-controlled pause state, halting DAG execution.
- Failed: Halted due to errors encountered during execution.
- Restart: Initiating DAG execution from the beginning.
- Re-scheduling: Re-executing the DAG based on the defined schedule.

### DAG options

- Shutdown: Gracefully stopping and terminating the DAG.
- Trigger: Manually triggering the execution of the DAG.
- Delete DAG: Removing the DAG and associated metadata.

## Cluster Activity


## Dataset


## Security


## Browse


## Admin


### AirFlow Connections

| Conn Id                     | Conn Type             |
| --------------------------- | --------------------- |
| airflow_db                  | mysql                 |
| aws_default                 | aws                   |
| azure_batch_default         | azure_batch           |
| azure_cosmos_default        | azure_cosmos          |
| azure_data_explorer_default | azure_data_explorer   |
| azure_data_lake_default     | azure_data_lake       |
| azure_default               | azure                 |
| cassandra_default           | cassandra             |
| databricks_default          | databricks            |
| dingding_default            | http                  |
| drill_default               | drill                 |
| druid_broker_default        | druid                 |
| druid_ingest_default        | druid                 |
| elasticsearch_default       | elasticsearch         |
| emr_default                 | emr                   |
| facebook_default            | facebook_social       |
| fs_default                  | fs                    |
| ftp_default                 | ftp                   |
| google_cloud_default        | google_cloud_platform |
| hive_cli_default            | hive_cli              |
| hiveserver2_default         | hiveserver2           |
| http_default                | http                  |
| impala_default              | impala                |
| kafka_default               | kafka                 |
| kubernetes_default          | kubernetes            |
| kylin_default               | kylin                 |
| leveldb_default             | leveldb               |
| livy_default                | livy                  |
| local_mysql                 | mysql                 |
| metastore_default           | hive_metastore        |
| mongo_default               | mongo                 |
| mssql_default               | mssql                 |
| mysql_default               | mysql                 |
| opsgenie_default            | http                  |
| oracle_default              | oracle                |
| oss_default                 | oss                   |
| pig_cli_default             | pig_cli               |
| pinot_admin_default         | pinot                 |
| pinot_broker_default        | pinot                 |
| postgres_default            | postgres              |
| presto_default              | presto                |
| redis_default               | redis                 |
| redshift_default            | redshift              |
| salesforce_default          | salesforce            |
| segment_default             | segment               |
| sftp_default                | sftp                  |
| spark_default               | spark                 |
| sqlite_default              | sqlite                |
| sqoop_default               | sqoop                 |
| ssh_default                 | ssh                   |
| tableau_default             | tableau               |
| tabular_default             | tabular               |
| trino_default               | trino                 |
| vertica_default             | vertica               |
| wasb_default                | wasb                  |
| webhdfs_default             | hdfs                  |
| yandexcloud_default         | yandexcloud           |






## Reference:

https://airflow.apache.org/docs/apache-airflow/2.1.1/dag-run.html

https://airflow.apache.org/docs/apache-airflow/2.1.1/dag-run.html