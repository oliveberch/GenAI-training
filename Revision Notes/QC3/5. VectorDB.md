 ## VectorDB Introduction

- **Vector Database:**
  - Indexes, stores, and manipulates high-dimensional vector data.
  - Suitable for the 80% of unstructured data that can't fit into a relational database (RD).

- **Key Features:**
  - Fast retrieval and similarity search.
  - CRUD operations.
  - Metadata filtering.
  - Horizontal scaling.

- **Vector Embedding:**
  - Conversion of data (audio, visual, documents) into numerical vectors.
  - Vectors: Magnitude and dimension for semantic searching.

- **Embedding Model:**
  - Core component in VectorDB.
  - Converts textual data into numerical vectors for efficient storage and querying.

- **Vector Index:**
  - Data structure for converting and retrieving vector data.

- **Vector Indexing:**
  - Conversion of unstructured data into vectors.
  - Organizing similar vectors together using indexing for swift querying.

- **Vector Index vs Vector Database:**
  - Data Management:
    - Performance and fault tolerance (sharding and replication).
    - Monitoring (resource usage, query performance, system health).
  - Metadata storage and filtering.
  - Scalability.
  - Real-time updates.
  - Backups and collections.
  - Ecosystem integration.
  - Data security and access control.

- **Vector Database Usecases:**
  1. Long-term memory for LLM (RAG).
  2. Semantic search & similarity search.
  3. Recommendation systems.
  4. Machine learning: Clustering and Classification.
  5. Anomaly detection (Rare events, IT Threats, Financial Fraud).

- **Why Vector Databases?**
  - Unstructured data in various forms.
  - Vector data solves the problem by converting diverse data forms into vectors.

- **Vector Database Operations:**
  1. **Indexing:**
     - VectorDB indexes vectors using algorithms.
     - Maps vectors to a data structure for faster retrieval.

  2. **Querying:**
     - Compares indexed query vector to existing indexed vectors.
     - Finds nearest neighbors.

  3. **Post Processing:**
     - In some cases, retrieved data is post-processed.
     - Includes re-ranking results using a different similarity measure.

- **Algorithms for Swift Querying:**
  - **Random Projection (RP):**
    - Dot product of high-dimensional vector matrix and low-dimensional random projection matrix.
    - Preserves similarity while reducing dimensions.

  - **Product Quantization (PQ):**
    - Lossy compression technique.
    - Involves splitting, training, encoding, and querying using codebooks.

  - **Locality-sensitive Hashing (LSH):**
    - Indexing technique for approximate nearest-neighbor search.
    - Hashes vectors using hashing functions.

  - **Hierarchical Navigable Small World (HNSW):**
    - Creates hierarchical, tree-like structures for similarity search.
    - Uses greedy routing to navigate through the structure.

---

- **Dimension:**
  - In Vector DB, dimension refers to the number of features or attributes in the vector representation of data. For example, if you're representing a document using vectors, each dimension might correspond to a specific word or concept. The higher the dimensionality, the more information is captured in the vector representation.

- **Indexing:**
  - Vector DB uses indexing to map vectors to a data structure that enables fast retrieval. The indexing process involves organizing vectors in a way that facilitates efficient similarity searches. Various algorithms like Random Projection, Product Quantization, Locality-sensitive Hashing, and Hierarchical Navigable Small World (HNSW) are used for this purpose.

*Question:*

- **Prepping Data for NLP:**
  - In the context of NLP, preparing data involves cleaning, tokenizing, and converting textual data into numerical vectors. Techniques like word embeddings (Word2Vec, GloVe) or more advanced methods like transformers (BERT, GPT) are commonly used. These embeddings capture semantic relationships and meaning in the text.

- **Dimensionality in Vector DB Indexing:**
  - Dimensionality in Vector DB is crucial during the indexing process. Higher-dimensional vectors may provide a more detailed representation of data but can also impact the efficiency of indexing and retrieval. The choice of dimensionality depends on the specific use case, the nature of the data, and the trade-off between accuracy and speed.

- **Dimensionality in Different Models:**
  - Different models may have different approaches to dimensionality. For example, traditional word embeddings like Word2Vec or GloVe often have fixed-dimensional vectors. In contrast, transformer-based models like BERT or GPT can handle variable-length sequences and generate contextual embeddings with a dynamic dimensionality based on the input text.


*Questions on Past Sessions:*

- **Different Distances in Use Case:**
  - **Cosine Similarity (Cos Theta):**
    - Range: -1 to 1
    - Denotes the cosine of the angle between two vectors.
    - Used for comparing the direction or orientation of vectors, irrespective of their magnitudes.

  - **L2 (Euclidean Distance):**
    - Range: 0 to infinity
    - Denotes the Euclidean distance between two vectors.
    - Measures the magnitude or length of the vector, providing a sense of spatial separation.

  - **Inner Product (Dot Product):**
    - Denotes the product of the magnitudes of the vectors and the cosine of the angle between them.
    - Combines information about direction and magnitude, offering a holistic measure of similarity.