**Hugging Face:**

A community platform to build, deploy and train machine learning models.

- **Functionality:**
  - Hosting models, NLPs, and datasets.
  - Providing spaces to train models.
  - Inference model through API endpoints.

- **Notable for:**
  - **transformers** library.




### Transformers 

A library that provides pre-trained models for a wide range of NLP tasks, including language translation, text generation, and sentiment analysis. It is built on top of PyTorch and TensorFlow and is known for its user-friendly API.

### Key Features:
**Ease of Use:** The library is designed to be user-friendly, making advanced models accessible to both beginners and experienced practitioners.

**Wide Range of Models:** Supports many architectures for tasks like text classification, information extraction, and more.

**Flexibility:** Easily switch between models and tasks, allowing for experimentation and fine-tuning.

**Costs:** Reduced compute costs.

**Integration:** Easy integration with popular deep learning frameworks.




### Transformers:

- A library of models for natural language processing (NLP) tasks developed by Hugging Face.
- Provides *pre-trained models* for various tasks such as text classification, translation, summarization, etc.
  
- **Implemented with:**
  - TensorFlow
  - PyTorch
  - JAX

- **Frameworks:**
  - TensorFlow: Developed by Google Brain.
  - PyTorch: Developed by Meta/Facebook.
  - JAX: Developed by DeepMind, known for speed.


### Pipeline in Transformers:

- Pipelines abstract complex code, offering a simple API for tasks like:
  - Named Entity Recognition
  - Masked Language Modeling
  - Sentiment Analysis
  - Feature Extraction
  - Question Answering

- **Tasks:**
  - Preprocesses text using a tokenizer.
  - Feeds preprocessed text to the model.

- **Specialization:**
  - Chooses tailored functions for ML models and data provision for training.
  
- **Library Used:**
  - AutoModelForSequenceClassification.

### Fine-tuning 
A process of taking a pre-trained model and further training (or "fine-tuning") it on a specific dataset for a specific task. 

This concept is central to how modern NLP models are used, as it allows for the customization of general-purpose language models to perform well on particular tasks or datasets.

### Key Features:
**Adaptability:** Fine-tuning allows pre-trained models to adapt to specific domains or tasks.

**Efficiency:** Reduces the need for training a model from scratch, saving time and computational resources (costs).

**Customization:** Enables customization of models to suit unique requirements of different NLP tasks.

**Improved Performance:** Fine-tuning often leads to better model performance on specific tasks compared to using generic pre-trained models.



### Fine-Tuning a Model:

- **Definition:**
  - Tuning an existing LLM model with new data.

- **Steps:**
  - Prepare dataset.
  - Load a pre-trained tokenizer, use it for dataset encoding.
  - Load a pre-trained model.
  - Utilize Trainer and Training Arguments (including training dataset, evaluation dataset, data collector, and tokenizer).

- **Note:**
  - An "epoch" refers to one complete pass through the entire training dataset.

---

### Implementation done in session:

- **Sentiment Analysis using Pipeline:**

  - Uses the `transformers` library to create a sentiment analysis pipeline.
  - Classifies the sentiment of the input text ("I am excited for my next trip.") as positive or negative.

- **Zero-Shot Classification Pipeline:**

  - Utilizes the zero-shot classification pipeline from `transformers`.
  - Specifies candidate labels ("Geographical", "Political", "General") for the input text ("Mt Everest is the tallest mountain in the world").

- **DistilBERT Model Setup:**

  - Imports and sets up an instance of the DistilBERT tokenizer and model for sequence classification.
  - Creates a sentiment analysis pipeline using the specified tokenizer and model.

- **Text Encoding and Decoding:**

  - Encodes the text "I am learning Gen AI" using the DistilBERT tokenizer.
  - Prints the encoded text and decodes it back to the original text.

- **IMDb Dataset Processing:**

  - Installs the `datasets` library.
  - Loads the IMDb dataset using `load_dataset("imdb")`.
  - Tokenizes the dataset using the BERT tokenizer.
  - Defines a function `tokenize_text` for tokenization.
  - Shuffles and selects a subset of the training and test datasets.

- **Model Fine-Tuning:**

  - Loads a pre-trained model (BERT) for sequence classification.
  - Defines and trains a model using the specified training arguments and datasets.
  - Saves the trained model to a specified directory.

- **Evaluation with scikit-learn:**

  - Installs the necessary libraries (`scikit-learn` and `evaluate`).
  - Loads an evaluation metric ("accuracy") from the `evaluate` module.
  - Defines a function `compute_metric` for computing the evaluation metric.
  - Sets up a Trainer for training the model and evaluates it on the test dataset.

- **Loading Pre-trained IMDb Model:**

  - Loads a pre-trained IMDb model from a specified directory.

- **Sentiment Analysis using IMDb Model:**
  - Uses the loaded IMDb model for sentiment analysis on two different movie reviews.
