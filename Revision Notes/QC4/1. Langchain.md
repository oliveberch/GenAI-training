# Langchain

- LangChain is an open-source library for building Large Language Model (LLM) applications.
- **Key Points**:
  - Supports Python and JavaScript.
  - Differentiates between LLMs and chat models.
  - Chat models use LLMs for a conversational approach with various supported messages.

## LLM and Chat Model

In Langchain, there is a small but significant difference between llms and chat models.

**LLM:** Large Language Model. Text in, text out API.

**Chat Model:** It uses LLM for a more conversational approach. It is an interface around messages. The current supported messages in Langchain are: SystemMessage, HumanMessage, AIMessage, FunctionMessage, and ChatMessage.

**Chain:** Chain is a composition of Langchain elements.

Example: A simple LLM Chain consists of Prompt | Model | Output Parser.
 
```bash
! pip install langchain langchain_experimental
```

### Prompt Template

It is an abstraction for a prompt template provided by LangChain to create and work with prompts.

[Prompt Templates](https://python.langchain.com/docs/modules/model_io/prompts/quick_start)

```python
# PromptTemplate
from langchain.prompts.prompt import PromptTemplate

template_string = "generate code for {text} in {language}"

prompt = PromptTemplate.from_template(template=template_string)

prompt.save('sample_template.json')
prompt.save('sample_template.yaml')
s_prompt = PromptTemplate.from_file('sample_template.json')
```

```python
from dotenv import load_dotenv
load_dotenv()
! pip install openai
```

### OpenAI LLM

```python
# Open AI LLM
from langchain.llms.openai import OpenAI

llm = OpenAI(model_name="gpt-3.5-turbo")
```

```bash
! echo HUGGINGFACEHUB_API_TOKEN=token >> .env
```

### HuggingFace API

```python
# HuggingFace API
from langchain.llms.huggingface_hub import HuggingFaceHub

llm = HuggingFaceHub(
    repo_id="google/flan-t5-xxl",
    model_kwargs={"temperature": 0.0, "max_length": 200}
)
```

### Chat Prompt Template

```python
# Chat Prompt Template
from langchain.prompts import ChatPromptTemplate

template = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        ("human", "Hi!")
    ]
)

template.schema()
```

### Chat Model - OpenAI

```python
# Chat Model - Open AI
from langchain.chat_models import ChatOpenAI

chat_model = ChatOpenAI(model_name="gpt-3.5-turbo")
```

### Chat Model - Deep Infra

```python
# Chat Model - Deep Infra
from langchain.llms.deepinfra import DeepInfra
from langchain_experimental.chat_models import Llama2Chat

llm = DeepInfra(
    model_id="meta-llama/Llama-2-70b-chat-hf"
)

llm.model_kwargs = {
    "temperature": 0.0
}

model = Llama2Chat(llm=llm)
```

### String Output Parser

```python
# String Output Parser
from langchain_core.output_parsers import StrOutputParser

output_parser = StrOutputParser()
```

### LLM Chain

```python
# LLM Chain
from langchain.chains import LLMChain

chain = LLMChain(
    llm=llm,
    prompt=prompt,
    output_parser=output_parser
)

chain.invoke({"text": "add_numbers", "language": "java"})
```


## LangChain Expression Language (LCEL)

- LangChain Expression Language is an easy way to compose chains together using a Runnable Interface.
- **Features**:
  - Provides streaming, async, and parallel execution.
  - Supports retries, fallbacks, input/output schemas, and integration with LangSmith.
  - Runnable Interface includes methods like `stream`, `invoke`, `batch`, `astream`, `ainvoke`, and `abatch`.

### Runnable Interface

- Runnable Interface provides a set of methods to define and execute operations/chains in LangChain.
- **Key Types**:
  1. **Runnable Parallel**: Defines and runs multiple values and operations in parallel.
  2. **Runnable Passthrough**: Takes input and passes it through.
  3. **Runnable Lambda**: Turns Python functions into pipe-compatible functions.

#### Code Reference:

```python
# Example of Runnable Parallel
from langchain.runnables import RunnableParallel

# Example of Runnable Passthrough
from langchain.runnables import RunnablePassthrough

# Example of Runnable Lambda
from langchain.runnables import RunnableLambda
```

### LLM Chain

- LLM Chain is a composition of LangChain elements, including LLMs, prompts, and output parsers.
- **Components**:
  - Utilizes LLM, prompt, and output parser.
  - Example: Code generation chain with prompt "generate code for {text} in {language}".

#### Code Reference:

```python
from langchain.chains import LLMChain

# Example of LLM Chain
chain = LLMChain(llm=my_llm, prompt=my_prompt, output_parser=my_output_parser)
```

### PDF Loader

- PDF Loader in LangChain loads and splits text from PDF documents.
- **Implementation**:
  - Uses PyPDFLoader to load and split text from a PDF file.
  - Resulting pages can be processed further in LangChain.

#### Code Reference:

```python
from langchain.document_loaders import PyPDFLoader

# Example of PDF  Loader
loader = PyPDFLoader('path/to/pdf/file')
pages = loader.load_and_split()
```

### Pinecone Vector DB

- **Definition**: Pinecone is a vector database for efficient storage and retrieval of embeddings.
- **Levels**:
  - Organization (Org) > Projects > Indexes.
- **Index Types**:
  1. Serverless
  2. Pod (with storage and performance optimization)

#### Code Reference:

```python
import pinecone
from pinecone import Pinecone

# Example of Pinecone Integration
client = Pinecone(api_key='your-api-key')
index = client.Index('your-index')
```

### Pinecone and Hugging Face Integration

- **Integration Steps**:
  - Set up Pinecone by installing the client.
  - Use Pinecone for indexing and querying embeddings.
  - Example code includes loading embeddings from a model and indexing with Pinecone.

#### Code Reference:

```python
# Example of Pinecone and Hugging Face Integration
from pinecone import Pinecone
from pinecone import PodSpec
from sentence_transformers import SentenceTransformer

# Code for embedding generation and indexing
# ...
```

### LangChain with Pinecone Retrieval

- **Implementation**:
  - Combines LangChain with Pinecone for a Retrieval Question-Answering (QA) system.
  - Uses Pinecone for efficient document retrieval based on embeddings.

#### Code Reference:

```python
from langchain.vectorstores import Pinecone
from langchain.chains import RetrievalQA

# Example of LangChain with Pinecone Retrieval
vectordb = Pinecone(index, embeddings.embed_query, "text")
retriever = vectordb.as_retriever()
chain = RetrievalQA.from_chain_type(llm=model, chain_type="stuff", retriever=retriever)
```

### Multi-Query Retriever

- **Definition**: Multi-Query Retriever in LangChain combines LLM and retriever for improved document relevance.
- **Usage**:
  - Enhances document relevance by incorporating LLM insights into the retrieval process.

#### Code Reference:

```python
from langchain.retrievers import MultiQueryRetriever

# Example of Multi-Query Retriever
retriever = MultiQueryRetriever.from_llm(retriever=my_retriever, llm=my_llm)
retriever.get_relevant_documents("the boy")
```

### Agents

- **Definition**: Agents in LangChain take dynamic actions using a language model.
- **Requirements**:
  1. Base LLM (Large Language Model)
  2. Tool/Tools
  3. System Message (Optional)
  4. Memory

### Tools

- **Types**:
  - Builtin Tools
  - Tool Kits
  - Custom Tools
- **Reference**: [LangChain Handbook](https://www.pinecone.io/learn/series/langchain/)

### Agent Initialization

1. **Load Tools:**
    ```python
    from langchain.agents import load_tools
    tools = load_tools([], llm=llm)
    ```

2. **Initialize Agent:**
    ```python
    from langchain.agents import initialize_agent, AgentType
    agent = initialize_agent(AgentType.ZERO_SHOT_REACT_DESCRIPTION, tools=tools, verbose=True, llm=model, handle_parsing_errors=True, max_iterations=3)
    ```

3. **Bind Agent:**
    ```python
    agent.bind(stop="Observation")
    ```

4. **Invoke Agent:**
    ```python
    agent.invoke({"what is log(67)"})
    ```

### Conversational Agent

- **Definition**: Facilitates dynamic interactions in conversations.
- **Components**:
  - Memory for conversation history.
  - Supports Redis or SQL for memory.

    ```python
    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)
    agent = ConversationalAgent.from_llm_and_tools(tools=tools, llm=model, system_mesage=system_message)
    ```

### Agent Executor

- **Definition**: Manages execution of a conversational agent with memory and tools.
- **Features**:
  - Handles parsing errors.
  - Configurable maximum iterations.

    ```python
    agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, memory=memory, handle_parsing_errors=True, verbose=True, max_iterations=3)
    agent_executor.invoke({"What is Brightspeed?"})
    ```

### Vector Database Integration

1. **Setup Pinecone:**
    ```python
    from pinecone import Pinecone
    key = os.getenv('PINECONE_API_KEY')
    client = Pinecone(api_key=key)
    index = client.Index('trng-index')
    ```

2. **Initialize HuggingFace Embeddings:**
    ```python
    from langchain.embeddings.huggingface import HuggingFaceEmbeddings
    embeddings = HuggingFaceEmbeddings(model_name='thenlper/gte-large')
    ```

3. **Create Tool for Vector Database Queries:**
    ```python
    from langchain.agents import Tool
    from langchain.vectorstores.pinecone import Pinecone
    from langchain.prompts import PromptTemplate

    vectordb = Pinecone(index, embedding=embeddings.embed_query, text_key="text")

    prompt = PromptTemplate.from_template(template="query the vector db for results")
    chain = prompt | llm

    tool = Tool(name="vector db tool", description="retrieve information related to Brightspeed", func=vectordb.similarity_search, retreiver_top_k=3)
    tools.append(tool)
    ```


## Sample Questions:


- What is LangChain, and what purpose does it serve in the field of language models?
- Can you explain the key features of LangChain?
- Differentiate between LLMs and chat models in LangChain.
- What are the supported messages in LangChain for chat models?
- How do you define a prompt template in LangChain?
- Can you explain the significance of the template in the context of LangChain?
- Explain the process of integrating OpenAI LLM in LangChain.
- How would you use the HuggingFace API as an LLM in LangChain?
- How do you create a chat prompt template in LangChain?
- What role does the chat prompt template play in facilitating conversations with the model?
- Describe the Runnable Interface in LangChain.
- When would you use methods like `stream`, `invoke`, `batch`, `astream`, `ainvoke`, and `abatch`?

- Walk through the components of an LLM Chain in LangChain.
- Provide an example of a scenario where an LLM Chain would be beneficial.
- Explain the purpose of the PDF Loader in LangChain.
- How does the PyPDFLoader split and load text from PDF documents?
- What is Pinecone, and how does it integrate with LangChain?
- Can you describe the process of indexing and querying embeddings using Pinecone?
- How do you integrate Pinecone with embeddings generated from a Hugging Face model in LangChain?
- Explain the concept of Retrieval Question-Answering (QA) in LangChain.
- How does LangChain use Pinecone for efficient document retrieval based on embeddings?
- What is the purpose of the Multi-Query Retriever in LangChain?
- How does it enhance document relevance by combining LLM and retriever?
- What are the key components of an agent in LangChain?
- How are tools utilized within the LangChain agent framework?
- How does a Conversational Agent facilitate dynamic interactions in LangChain?
- What role does memory play in the conversation history?
- Explain the responsibilities of an Agent Executor in LangChain.
- How does it manage the execution of a conversational agent with memory and tools?
- Walk through the steps of setting up Pinecone for vector database integration in LangChain.
- How would you use Hugging Face embeddings to create a tool for vector database queries?
