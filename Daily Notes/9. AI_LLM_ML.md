## Minutes of session

**AI:**
development of computer systems that can perform tasks that typically require human intelligence
- Subset: Machine Learning (ML) and Generative AI (GenAI).

**Machine Learning (ML):**
development of algorithms and models that enable computers to learn from data, instead of relying on explicit programming.
- Equation: \(f(x) = wx + b\).
- Model Training: Requires data (supervised or unsupervised).
- Types:
  - Supervised: Uses labeled data.
  - Unsupervised: Model trained on unlabeled data.
    - Self-Supervised Learning: ML model creates its own Large Language Model (LLM).
  - Semi-Supervised.
  - Reinforcement.
  - DPO: direct preference optimization, adds a layer to model just for a reward system.


**Transfer Learning in Machine Learning:**
- Definition: Reusing a pre-trained model on a new problem.
- Application: Pre-trained models leverage knowledge gained from a previous task to improve generalization on a new task.

**Predictive AI vs. Discriminative AI vs. Generative AI:**
- Predictive AI:   
  - Forecast future outcomes based on historical data.
  - Eg. algorithms analyze past market trends to predict future stock prices.
- Discriminative AI: 
  - Categorizes data into distinct classes or categories.
  - Eg. a model distinguishe between different objects or entities in images.
- Generative AI: 
  - Generates images, texts, sounds.
  - Eg. Generative Adversarial Networks (GANs) in computer vision, which generate realistic images by learning from a dataset

**Large Language Models (LLM):**
- Takes input and predicts the next set of tokens.
- Example Models: GPT, Claude, Gemini, Lama.
- Application: understanding of Natural Language, text generation, content summarization, chatbots, language translation

**Hugging Face:**
- Origin: Started as a chatbot for teens.
- Evolution: Pivoted into an open-source platform.
- Functionality:
  - Hosting models, NLPs, and datasets.
  - Providing spaces to train models.
  - Inference through API endpoints.
- Notable for the **transformers** library.


**Transformers**
- A library of models for natural language processing (NLP) tasks developed by Hugging face
- provides *pre-trained models* for various tasks such as text classification, translation, summarization, and more.

 - models provided by transformers are implemented using either TensorFlow or PyTorch or JAX
 - tensorflow and pytorch are both open-source machine learning (ML) frameworks
 - tensorflow was developed by google brains
 - pytorch was developed by Meta / facebook
 - JAX is a machine learning framework for transforming numerical functions by deepmind, it's much faster


**Pipeline in Higginig face:**
pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks like:
- Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering, etc.

- tasks done by pipeline:
  - Preprocesses text using a tokenizer.
  - Feeds the preprocessed text to the model.

- Specialization: chooses tailored func for ML models and data provision for training.
- Library Used: AutoModelForSequenceClassification.


**Fine-Tuning a Model:**
- Definition: Tuning an existing LLM model with new data.
- Steps:
  - Prepare dataset.
  - Load a pre-trained tokenizer, use it for dataset encoding.
  - Load a pre-trained model.
  - Utilize Trainer and Training Arguments (including training dataset, evaluation dataset, data collector, and tokenizer).
- Note: An "epoch" refers to one complete pass through the entire training dataset.
 
## Implementation done in session:

* **Sentiment Analysis Pipeline:**
  * Uses the `transformers` library to create a sentiment analysis pipeline.
  * Classifies the sentiment of the input text ("I am excited for my next trip.") as positive or negative.

* **Zero-Shot Classification Pipeline:**
  * Utilizes the zero-shot classification pipeline from `transformers`.
  * Specifies candidate labels ("Geographical", "Political", "General") for the input text ("Mt Everest is the tallest mountain in the world").

* **DistilBERT Model Setup:**
  * Imports and sets up an instance of the DistilBERT tokenizer and model for sequence classification.
  * Creates a sentiment analysis pipeline using the specified tokenizer and model.

* **Text Encoding and Decoding:**
  * Encodes the text "I am learning Gen AI" using the DistilBERT tokenizer.
  * Prints the encoded text and decodes it back to the original text.

* **IMDb Dataset Processing:**
  * Installs the `datasets` library.
  * Loads the IMDb dataset using `load_dataset("imdb")`.
  * Tokenizes the dataset using the BERT tokenizer.
  * Defines a function `tokenize_text` for tokenization.
  * Shuffles and selects a subset of the training and test datasets.

* **Model Fine-Tuning:**
  * Loads a pre-trained model (BERT) for sequence classification.
  * Defines and trains a model using the specified training arguments and datasets.
  * Saves the trained model to a specified directory.

* **Evaluation with scikit-learn:**
  * Installs the necessary libraries (`scikit-learn` and `evaluate`).
  * Loads an evaluation metric ("accuracy") from the `evaluate` module.
  * Defines a function `compute_metric` for computing the evaluation metric.
  * Sets up a Trainer for training the model and evaluates it on the test dataset.

* **Loading Pre-trained IMDb Model:**
  * Loads a pre-trained IMDb model from a specified directory.

* **Sentiment Analysis using IMDb Model:**
  * Uses the loaded IMDb model for sentiment analysis on two different movie reviews.
