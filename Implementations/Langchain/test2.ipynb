{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "template_string = \"generate code for {text} in {language}\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=template_string)\n",
    "\n",
    "prompt.save('sample_template.json')\n",
    "# prompt.save('sample_template.yaml')\n",
    "s_prompt = PromptTemplate.from_file('sample_template.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'ChatPromptTemplate',\n",
       " 'description': 'A prompt template for chat models.\\n\\nUse to create flexible templated prompts for chat models.\\n\\nExamples:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.prompts import ChatPromptTemplate\\n\\n        template = ChatPromptTemplate.from_messages([\\n            (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\\n            (\"human\", \"Hello, how are you doing?\"),\\n            (\"ai\", \"I\\'m doing well, thanks!\"),\\n            (\"human\", \"{user_input}\"),\\n        ])\\n\\n        messages = template.format_messages(\\n            name=\"Bob\",\\n            user_input=\"What is your name?\"\\n        )',\n",
       " 'type': 'object',\n",
       " 'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "  'input_variables': {'title': 'Input Variables',\n",
       "   'type': 'array',\n",
       "   'items': {'type': 'string'}},\n",
       "  'input_types': {'title': 'Input Types', 'type': 'object'},\n",
       "  'output_parser': {'$ref': '#/definitions/BaseOutputParser'},\n",
       "  'partial_variables': {'title': 'Partial Variables', 'type': 'object'},\n",
       "  'metadata': {'title': 'Metadata', 'type': 'object'},\n",
       "  'tags': {'title': 'Tags', 'type': 'array', 'items': {'type': 'string'}},\n",
       "  'messages': {'title': 'Messages',\n",
       "   'type': 'array',\n",
       "   'items': {'anyOf': [{'$ref': '#/definitions/BaseMessagePromptTemplate'},\n",
       "     {'$ref': '#/definitions/BaseMessage'},\n",
       "     {'$ref': '#/definitions/BaseChatPromptTemplate'}]}},\n",
       "  'validate_template': {'title': 'Validate Template',\n",
       "   'default': False,\n",
       "   'type': 'boolean'}},\n",
       " 'required': ['input_variables', 'messages'],\n",
       " 'definitions': {'BaseOutputParser': {'title': 'BaseOutputParser',\n",
       "   'description': 'Base class to parse the output of an LLM call.\\n\\nOutput parsers help structure language model responses.\\n\\nExample:\\n    .. code-block:: python\\n\\n        class BooleanOutputParser(BaseOutputParser[bool]):\\n            true_val: str = \"YES\"\\n            false_val: str = \"NO\"\\n\\n            def parse(self, text: str) -> bool:\\n                cleaned_text = text.strip().upper()\\n                if cleaned_text not in (self.true_val.upper(), self.false_val.upper()):\\n                    raise OutputParserException(\\n                        f\"BooleanOutputParser expected output value to either be \"\\n                        f\"{self.true_val} or {self.false_val} (case-insensitive). \"\\n                        f\"Received {cleaned_text}.\"\\n                    )\\n                return cleaned_text == self.true_val.upper()\\n\\n                @property\\n                def _type(self) -> str:\\n                        return \"boolean_output_parser\"',\n",
       "   'type': 'object',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'}}},\n",
       "  'BaseMessagePromptTemplate': {'title': 'BaseMessagePromptTemplate',\n",
       "   'description': 'Base class for message prompt templates.',\n",
       "   'type': 'object',\n",
       "   'properties': {}},\n",
       "  'BaseMessage': {'title': 'BaseMessage',\n",
       "   'description': 'The base abstract Message class.\\n\\nMessages are the inputs and outputs of ChatModels.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type', 'type': 'string'}},\n",
       "   'required': ['content', 'type']},\n",
       "  'BaseChatPromptTemplate': {'title': 'BaseChatPromptTemplate',\n",
       "   'description': 'Base class for chat prompt templates.',\n",
       "   'type': 'object',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'input_variables': {'title': 'Input Variables',\n",
       "     'type': 'array',\n",
       "     'items': {'type': 'string'}},\n",
       "    'input_types': {'title': 'Input Types', 'type': 'object'},\n",
       "    'output_parser': {'$ref': '#/definitions/BaseOutputParser'},\n",
       "    'partial_variables': {'title': 'Partial Variables', 'type': 'object'},\n",
       "    'metadata': {'title': 'Metadata', 'type': 'object'},\n",
       "    'tags': {'title': 'Tags', 'type': 'array', 'items': {'type': 'string'}}},\n",
       "   'required': ['input_variables']}}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chat Prompt Template\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"Hi!\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "template.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Revature Training\\Daily Workspaces\\7_11thFeb\\venv\\Lib\\site-packages\\langchain_community\\llms\\openai.py:249: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "d:\\Revature Training\\Daily Workspaces\\7_11thFeb\\venv\\Lib\\site-packages\\langchain_community\\llms\\openai.py:1070: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Open AI LLM\n",
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Model - Open AI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace API\n",
    "from langchain.llms.huggingface_hub import HuggingFaceHub\n",
    "\n",
    "llm2 = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-xxl\",\n",
    "    model_kwargs={\"temperature\": 1.0, \"max_length\": 200}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Model - Deep Infra\n",
    "from langchain.llms.deepinfra import DeepInfra\n",
    "from langchain_experimental.chat_models import Llama2Chat\n",
    "\n",
    "llm3 = DeepInfra(\n",
    "    model_id=\"meta-llama/Llama-2-70b-chat-hf\"\n",
    ")\n",
    "\n",
    "llm3.model_kwargs = {\n",
    "    \"temperature\": 0.0\n",
    "}\n",
    "\n",
    "model = Llama2Chat(llm=llm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String Output Parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'd = int(input()) if d  0: d = d / 100 if d  0: d = d / 100 * d / 100 if d  0: d = d / 100 * d / 100 if d  0: d = d / 100 * d / 100 if d',\n",
       " 'language': 'python'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM Chain\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm2,\n",
    "    prompt=prompt,\n",
    "    output_parser=output_parser\n",
    ")\n",
    "\n",
    "chain.invoke({\"text\": \"function to take distance in meters and return in centimeter\", \"language\": \"python\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if x%2==0: print(x) else: print(x%3)'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LLM chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | llm2 | parser\n",
    "\n",
    "chain.invoke({'text':'print prime numbers', 'language':'python'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company_name': AIMessage(content='  Sure, based on your description, the company name that comes to mind is \"Terrafugia\". Terrafugia is a company that specializes in developing and manufacturing flying cars, also known as personal aerial vehicles (PAVs). They aim to create practical and safe flying cars that can be used for personal transportation and other applications.'),\n",
       " 'company_slogan': '  Sure, here\\'s a slogan for Terrafugia:\\n\\n\"Terrafugia: Take to the skies, on your own terms.\"\\n\\nHere\\'s a breakdown of the slogan:\\n\\n* \"Take to the skies\" refers to the ability of Terrafugia\\'s flying cars to take off and land vertically, allowing users to travel by air.\\n* \"on your own terms\" emphasizes the personal nature of the vehicles, which are designed to be practical and safe for individual use.\\n\\nI hope this slogan captures the essence of Terrafugia\\'s mission and products! Let me know if you have any other questions or requests.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LLM Sequential Chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "company_name_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are an assistant that provides a single company name based on the description provided by the user\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "slogan_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"genrate a slogan from company name provided by the user and print it along with the name of the company\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{company_name}\")\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    company_name = company_name_prompt | model,\n",
    "    company_slogan = {'company_name' : company_name_prompt | model} | slogan_prompt_template | model | parser \n",
    ")\n",
    "# chain = {'company_name' : company_name_prompt | model} | slogan_prompt_template | model | parser \n",
    "\n",
    "chain.invoke({'text':'a company that sells flying cars'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code_generation': AIMessage(content=\"  Sure! Here's an example of how you could use a for loop to print all prime numbers up to a certain number in Python:\\n```\\nn = 30\\nfor i in range(2, n+1):\\n    is_prime = True\\n    for j in range(2, int(i**0.5) + 1):\\n        if i % j == 0:\\n            is_prime = False\\n            break\\n    if is_prime:\\n        print(i)\\n```\\nThis code uses a nested for loop to check whether a number is prime. The outer loop iterates from 2 to n, and the inner loop iterates from 2 to the square root of the current number being checked. If the number is divisible by any number in the inner loop, it is not prime, and the inner loop breaks. If the number is not divisible by any number in the inner loop, it is prime, and it is printed.\\n\\nYou can also use the Sieve of Eratosthenes algorithm to find all prime numbers up to a certain number. Here's an example of how you could implement that in Python:\\n```\\ndef sieve_of_eratosthenes(n):\\n    is_prime = [True] * (n + 1)\\n    for p in range(2, int(n**0.5) + 1):\\n        if is_prime[p]:\\n            for i in range(p**2, n + 1, p):\\n                is_prime[i] = False\\n    return [p for p in range(2, n + 1) if is_prime[p]]\\n```\\nThis function creates a list of booleans, where each element represents whether a number is prime or not. The function then iterates over the primes, and marks as composite (i.e. sets to False) all the multiples of each prime, using the inner loop. Finally, it returns a list of all the primes.\\n\\nYou can call this function by passing in the maximum number you want to check, like this:\\n```\\nprint(sieve_of_eratosthenes(30))\\n```\\nThis would print all prime numbers up to 30.\\n\\nI hope this helps! Let me know if you have any questions.\"),\n",
       " 'code_complexity': AIMessage(content=\"  Sure, I'd be happy to help! The code you provided is a great example of how to find prime numbers using the Sieve of Eratosthenes algorithm.\\n\\nIn terms of time complexity, the Sieve of Eratosthenes algorithm has a time complexity of O(n log log n), where n is the maximum number being checked. This is because the algorithm only needs to check the primes up to the square root of n, and it can skip over many composite numbers quickly by marking them as composite based on their multiples.\\n\\nIn terms of space complexity, the algorithm uses a list of booleans to keep track of whether a number is prime or not, which has a space complexity of O(n).\\n\\nOverall, the Sieve of Eratosthenes algorithm is a very efficient way to find prime numbers, with a time complexity of O(n log log n) and a space complexity of O(n).\\n\\nI hope this helps! Let me know if you have any other questions.\"),\n",
       " 'code_comparision': AIMessage(content=\"  Sure, I'd be happy to help! The code you provided is a great example of how to find prime numbers using the Sieve of Eratosthenes algorithm.\\n\\nIn terms of time complexity, the Sieve of Eratosthenes algorithm has a time complexity of O(n log log n), where n is the maximum number being checked. This is because the algorithm only needs to check the primes up to the square root of n, and it can skip over many composite numbers quickly by marking them as composite based on their multiples.\\n\\nIn terms of space complexity, the algorithm uses a list of booleans to keep track of whether a number is prime or not, which has a space complexity of O(n).\\n\\nOverall, the Sieve of Eratosthenes algorithm is a very efficient way to find prime numbers, with a time complexity of O(n log log n) and a space complexity of O(n).\\n\\nI hope this helps! Let me know if you have any other questions.\")}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM Sequential Chain\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "code_generation_prompt = ChatPromptTemplate.from_messages(\n",
    "   [\n",
    "       SystemMessage(content=\"You are a code assistant that generates code for users input\"),\n",
    "       HumanMessagePromptTemplate.from_template(\"generate code for {text} in {language}\")\n",
    "\n",
    "   ] \n",
    ")\n",
    "\n",
    "time_complexity_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a code assistant that evaluates the time and space complexity of code for users input\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{code}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "code_summary_chain = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"you are a code assitant that prints the time complexity of the code and code provided by the user and an explanation for the code\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{code} and {time_complexity} \")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    code_generation = code_generation_prompt | model,\n",
    "    code_complexity = RunnablePassthrough.assign(code = (code_generation_prompt | model))| time_complexity_prompt | model,\n",
    "    code_comparision = {'code' : (code_generation_prompt | model), 'time_complexity': ({'code' : code_generation_prompt | model} | time_complexity_prompt | model)} | code_summary_chain | model\n",
    ")\n",
    "\n",
    "\n",
    "chain.invoke({'text':'print prime numbers', 'language':'python'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"  Hello! Welcome to Brightspeed Support. I'd be happy to help you with information about our broadband plans.\\n\\nWe offer a variety of broadband plans to fit your needs, with different speeds and data caps to choose from. Our plans are designed to provide you with reliable and fast internet connectivity for all your online activities, from browsing and streaming to online gaming and remote work.\\n\\nHere are some of the broadband plans we currently offer:\\n\\n1. Basic: This plan offers speeds of up to 50 Mbps and a data cap of 500 GB. It's perfect for light internet users who primarily use the internet for browsing, emailing, and social media.\\n2. Standard: Our most popular plan, it offers speeds of up to 100 Mbps and a data cap of 1 TB. It's ideal for households with multiple devices and moderate internet usage, including streaming and online gaming.\\n3. Turbo: This plan offers speeds of up to 200 Mbps and a data cap of 2 TB. It's great for heavy internet users who enjoy streaming high-definition content, online gaming, and remote work.\\n4. Ultimate: Our top-tier plan offers speeds of up to 500 Mbps and a data cap of 5 TB. It's perfect for households with multiple devices and very high internet usage, including 4K streaming and heavy online gaming.\\n\\nWe also offer customized plans for businesses, so please let me know if you have any specific requirements for your business.\\n\\nPlease note that the availability and pricing of these plans may vary depending on your location. I can help you check the availability and pricing of these plans in your area if you provide me with your zip code or city and state.\\n\\nWhich plan sounds like the best fit for your needs?\")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM Router chain\n",
    "\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# from transformers import pipeline\n",
    "\n",
    "sales_prompt = ChatPromptTemplate.from_messages(\n",
    "\n",
    "    [\n",
    "\n",
    "    SystemMessage(content=\"You are a helpfull, honest sales assisnt for a telecom company called brightspeed\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "\n",
    "    ]\n",
    "    \n",
    ")\n",
    "\n",
    "service_prompt = ChatPromptTemplate.from_messages(\n",
    "\n",
    "    [\n",
    "    SystemMessage(content=\"You are a helpfull, honest service assisnt for a telecom company called brightspeed guide the user to resolve the issues.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "router_prompt = ChatPromptTemplate.from_messages(\n",
    "\n",
    "    [\n",
    "    SystemMessage(content=\"Categorize the user input to sales or service along with actual text\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# classifier = pipeline('zero-shot-classification', candidate_labels = ['sales', 'service'])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = router_prompt | model | parser\n",
    "\n",
    "def categorize_text(text):\n",
    "    if(\"sales\" in text):\n",
    "        return sales_prompt\n",
    "    else:\n",
    "        return service_prompt\n",
    "    \n",
    "\n",
    "router_chain = RunnablePassthrough.assign(text = (router_prompt | model | parser)) | RunnableLambda(lambda text: categorize_text(text)) | model\n",
    "\n",
    "router_chain.invoke({'text':\"What are the broadband plans available?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='  Sure, I\\'d be happy to help! The actual text you provided is:\\n\\n\"ITS ME HI!\"\\n\\nIs there anything else I can assist you with?')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM Transform Chain\n",
    "\n",
    "def to_upper(input):\n",
    "    return {'text':str(input).upper()}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"you are a helpfull assistant\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}, print the actual text\")\n",
    "    ]\n",
    "    \n",
    ") \n",
    "\n",
    "chain = RunnableLambda(lambda text: to_upper(text)) | prompt | model\n",
    "\n",
    "chain.invoke({\"text\":\"Its me Hi!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': {'What is 2+2?'}, 'answer': 'Answer:  4'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM Math chain\n",
    "\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "math_chain = LLMMathChain.from_llm(llm2)\n",
    "\n",
    "math_chain.invoke({\"What is 2+2?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Example of PDF  Loader\n",
    "loader = PyPDFLoader('the-velveteen-rabbit.pdf')\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2107, which is longer than the specified 1000\n",
      "Created a chunk of size 1625, which is longer than the specified 1000\n",
      "Created a chunk of size 1857, which is longer than the specified 1000\n",
      "Created a chunk of size 1859, which is longer than the specified 1000\n",
      "Created a chunk of size 1862, which is longer than the specified 1000\n",
      "Created a chunk of size 1276, which is longer than the specified 1000\n",
      "Created a chunk of size 1376, which is longer than the specified 1000\n",
      "Created a chunk of size 1984, which is longer than the specified 1000\n",
      "Created a chunk of size 1767, which is longer than the specified 1000\n",
      "Created a chunk of size 1945, which is longer than the specified 1000\n",
      "Created a chunk of size 1599, which is longer than the specified 1000\n",
      "Created a chunk of size 1423, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "# text spliting using character splitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 50,\n",
    "    separator='\\n'\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# embedding model from hf\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name='thenlper/gte-large'\n",
    ")\n",
    "\n",
    "index = 'index1'\n",
    "\n",
    "result = [ Pinecone.from_documents(pages,embeddings.embed_documents, index_name =index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pinecone\n",
    "from langchain.vectorstores.pinecone import Pinecone\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "key = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "client = Pinecone(index = \"index1\", text_key = key, embedding = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.pinecone import Pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "\n",
    "client = Pinecone(api_key=key)\n",
    "\n",
    "index = client.Index('index1')\n",
    "\n",
    "vectordb = Pinecone(index, embeddings.embed_query, \"text\")\n",
    "\n",
    "retreiver = vectordb.as_retriever()\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=model, chain_type=\"stuff\", retriever=retreiver)\n",
    "\n",
    "chain.run(\"rabbit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retreiver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Multi Query Retrival\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrievers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiQueryRetriever\n\u001b[0;32m      5\u001b[0m retreiver \u001b[38;5;241m=\u001b[39m MultiQueryRetriever\u001b[38;5;241m.\u001b[39mfrom_llm(\n\u001b[1;32m----> 6\u001b[0m     retriever\u001b[38;5;241m=\u001b[39m \u001b[43mretreiver\u001b[49m,\n\u001b[0;32m      7\u001b[0m     llm \u001b[38;5;241m=\u001b[39m llm2\n\u001b[0;32m      8\u001b[0m     )\n\u001b[0;32m     10\u001b[0m retreiver\u001b[38;5;241m.\u001b[39mget_relevant_documents(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe boy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'retreiver' is not defined"
     ]
    }
   ],
   "source": [
    "# Multi Query Retrival\n",
    "\n",
    "from langchain.retrievers import MultiQueryRetriever\n",
    "\n",
    "retreiver = MultiQueryRetriever.from_llm(\n",
    "    retriever= retreiver,\n",
    "    llm = llm2\n",
    "    )\n",
    "\n",
    "retreiver.get_relevant_documents(\"the boy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'function to take distance in meters and return in centimeter',\n",
       " 'language': 'python',\n",
       " 'text': 'm = int(input()) c = m / 100 if m == 0: c = 0 else: c = c + 1 if c == 0: c = c + 1 if c == 0: c = c + 1 if c == 0: c = c + 1 if c == 0: c'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
